# -*- coding: utf-8 -*-
"""userUploadSentimentAnalysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mETDpa1SG3lCgP6G_KYWyy5ES05ANFkh
"""

import requests
from bs4 import BeautifulSoup
import urllib.request
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import csv
#for stop words removal on training data
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

#for text preprocessing and model building
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Flatten, Dense, Dropout, GlobalMaxPooling1D, Embedding, Bidirectional, Conv1D, LSTM

#for wordCloud
from wordcloud import WordCloud

#gives most repetitive element in list
from collections import Counter

#pip install transformers
from transformers import pipeline
pipe = pipeline(model="facebook/bart-large-mnli")
import spacy
from spacy import displacy
NER = spacy.load("en_core_web_sm")

import numpy as np

def remove_stopwords(input_text):

    stopwords_list = stopwords.words('english')
    # Some words which might indicate a certain sentiment are kept via a whitelist
    whitelist = ["n't", "not", "no"]
    words = input_text.split()
    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1]
    return " ".join(clean_words)

def tokenizingSequencingPadding(myList):
  oov_token  = '#OOV'
  max_length = 100
  trunc_type = 'post'
  pad_type   = 'pre'
  vocab_size = 10000
  embedding_dim = 100
  tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_token)
  tokenizer.fit_on_texts(myList)
  testing_sequence = tokenizer.texts_to_sequences(myList)
  padded_testing_seq = pad_sequences(testing_sequence, maxlen = max_length, padding = pad_type, truncating = trunc_type) # type numpy.ndarray
  return padded_testing_seq

def preprocess(paragraph):
  noStopwordsPara = remove_stopwords(paragraph)
  preprocessedText=tokenizingSequencingPadding(noStopwordsPara)
  return preprocessedText

def wordCloudGeneration(data):
  wordForCloud = " ".join(title for title in data.Title)
  word_cloud = WordCloud(collocations = False, background_color = 'white').generate(wordForCloud)
  word_cloud.to_file("wordCloud.png")
  plt.imshow(word_cloud, interpolation='bilinear')
  plt.axis("off")
  plt.show()

def userUpload(stockTicker,isUrl,isText,isFile,url,text,file):
  if isUrl==True:
    sentiment = userUploadUrl(url)

  elif isText==True:
    #NER
    organization_list=[]
    outputNER= NER(text)
    for word in outputNER.ents:
      if word.label_ == 'ORG':
        organization_list.append(word.text)
    counter = Counter(organization_list)
    most_common = counter.most_common(1)[0][0]

    # removing stop words
    sw_removed = remove_stopwords(text)

    # generating word cloud
    wordForCloud = " ".join(word for word in text.split())
    word_cloud = WordCloud(collocations = False, background_color = 'white').generate(wordForCloud)
    word_cloud.to_file("textWordCloud.png")

    # making a list for tokanization
    tempList=[]
    tempList.append(sw_removed)
    inputY = tokenizingSequencingPadding(tempList)

    # loading the LSTM model
    lstm_model=tf.keras.models.load_model('lstmModel.keras')

    # prediction
    lstm_output = lstm_model.predict(inputY)
    print(lstm_output)
    #[negative,neutral,positive]
    result={
        "negative":lstm_output[0][0],
        "neutral":lstm_output[0][1],
        "positive":lstm_output[0][2],
        "company":most_common
    }

    return result
  else:

    if isFile == True:

      #get the file uploaded by user and remove stopwords
      data=pd.read_csv(file)

      data['reviews.text']=data['reviews.text'].apply(remove_stopwords)

      print("stopwords removal complete")

      # Generate a word cloud image
      wordForCloud = " ".join(record for record in data['reviews.text'])
      print(wordForCloud)
      word_cloud = WordCloud(collocations = False, background_color = 'white').generate(wordForCloud)
      word_cloud.to_file("reviewWordCloud.png")

      print("wordcloud save complete")

      lstmModel = tf.keras.models.load_model('lstmModel.keras')
      print("loding model complete")

      #text preprocessing of uploaded file
      #tokenization, sequencing, padding
      sentancesForPrediction = data['reviews.text'].to_list()
      padded_sentancesForPrediction = tokenizingSequencingPadding(sentancesForPrediction)
      print("padding done")

      #prediction
      lstm_model_output = lstmModel.predict(padded_sentancesForPrediction[:])
      print("prediction complete")

      # [negative,neutral,positive]
      prediction_list=[]
      positive=0
      negative=0
      neutral=0
      for i in lstm_model_output:
        # print(i)
        maximum=(max(i))
        # print(maximum)
        index = np.where( i == maximum)[0][0]
        # print(index)
        if index==0:
          prediction_list.append("negative")
          negative = negative+1
        elif index==1:
          prediction_list.append("neutral")
          neutral = neutral+1
        else:
          prediction_list.append("positive")
          positive = positive+1

      print("POSITIVE : ",positive,"NEGATIVE",negative,"NEUTRAL",neutral)
      print(len(sentancesForPrediction))
      percentagePositive = (positive/len(sentancesForPrediction))*100
      percentageNegative = (negative/len(sentancesForPrediction))*100
      percentageNeutral = (neutral/len(sentancesForPrediction))*100

      result={
          "negative" : percentageNegative,
          "neutral":percentageNeutral,
          "positive":percentagePositive,
          "company":""
          }
      return result


      # return [percentagePositive,percentageNegative,percentageNeutral]

fileUpload = userUpload("",False,False,True,"","","file1 (1).csv")
print("OUTPUT")
print(fileUpload)

text = userUpload("Air India",False,True,False,"","Air India shares first look of plane after logo rebranding. See pics","")
print("OUTPUT")
print(text)

type(fileUpload)

# def userUploadUrl(url):

#   response = requests.get(url)

#   # Check if the request was successful
#   if response.status_code == 200:
#     html_content = response.content
#   else:
#     print("Failed to fetch the website.")
#     return

#   soup = BeautifulSoup(html_content, 'html.parser')

#   # Find all the text elements (e.g., paragraphs, headings, etc.) you want to scrape
#   text_elements = soup.find_all(['h1'])

#   # Extract the text from each element and concatenate them into a single string
#   scraped_text = ' '.join(element.get_text() for element in text_elements)

#   print(scraped_text)

# import re

# # User input: Enter the URL of the website you want to scrape
# url = input("Enter the URL of the website: ")

# # Send an HTTP GET request
# response = requests.get(url)

# # Check if the request was successful (status code 200)
# if response.status_code == 200:
#     # Parse the HTML content of the page
#     soup = BeautifulSoup(response.content, 'html.parser')

#     # Define regular expressions for common ad-related patterns
#     ad_patterns = [
#         re.compile(r'\b(?:ad|sponsored|promo|advertisement|custom|)\b', flags=re.IGNORECASE), # Common keywords
#         re.compile(r'ad-', flags=re.IGNORECASE), # Class names with 'ad-'
#         re.compile(r'promo-', flags=re.IGNORECASE), # Class names with 'promo-'
#     ]

#     # Find and remove elements containing ad-related patterns or class names
#     for pattern in ad_patterns:
#         for element in soup.find_all():
#             if element.has_attr('class') and any(pattern.search(cls) for cls in element['class']):
#                 element.extract()

#     # Now you can proceed to extract relevant data from the cleaned HTML

#     # For demonstration purposes, let's print the remaining content

#     for div in soup.find_all('div'):
#         if all(a.name == 'a' for a in div.find_all()) and not div.find_all(text=True, recursive=False):
#             div.extract()

#     print(soup.prettify())

# else:
#     print("Failed to retrieve the web page.")

urlUpload = userUpload(True,False,False,"https://timesofindia.indiatimes.com/business/india-business/government-to-offer-most-nh-projects-on-ppp-mode/articleshow/104055241.cms","","")

data=pd.read_csv("/content/7817_1[1].csv")

data["reviews.text"].to_csv("file1.csv")

